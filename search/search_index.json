{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Database for Simplon Purpose The objective of this project is to provide a database for Simplon. Simplon wants to create a database to compare its training programs with those of other training centers. This will enable employees to understand which training programs face strong competition and provide potential learners with alternatives if Simplon's programs are full. Dependencies: poetry psycopg2-binary python-dotenv scrapy SQLAlchemy azure CLI azure ml CLI psql fastapi mkdocstrings-python Setup Create virtual environement and install requierements: poetry install Create .env file using env_template.txt To create the Azure ressources, execute these tasks: open the file ./azure_resources/1_azure_resources in the section VARIABLES , choose your own designations for the resources execute these commands: chmod +x ./azure_resources/1_azure_resources.sh ./azure_resources/1_azure_resources.sh Check the resources and execution of the function on the Azure portal Launch the API Get your token: poetry run python -m api.utils Update model_name in ./api/launch_app.sh and then you can execute it","title":"Accueil"},{"location":"#database-for-simplon","text":"","title":"Database for Simplon"},{"location":"#purpose","text":"The objective of this project is to provide a database for Simplon. Simplon wants to create a database to compare its training programs with those of other training centers. This will enable employees to understand which training programs face strong competition and provide potential learners with alternatives if Simplon's programs are full.","title":"Purpose"},{"location":"#dependencies","text":"poetry psycopg2-binary python-dotenv scrapy SQLAlchemy azure CLI azure ml CLI psql fastapi mkdocstrings-python","title":"Dependencies:"},{"location":"#setup","text":"Create virtual environement and install requierements: poetry install Create .env file using env_template.txt To create the Azure ressources, execute these tasks: open the file ./azure_resources/1_azure_resources in the section VARIABLES , choose your own designations for the resources execute these commands: chmod +x ./azure_resources/1_azure_resources.sh ./azure_resources/1_azure_resources.sh Check the resources and execution of the function on the Azure portal","title":"Setup"},{"location":"#launch-the-api","text":"Get your token: poetry run python -m api.utils Update model_name in ./api/launch_app.sh and then you can execute it","title":"Launch the API"},{"location":"Documentation/introduction/","text":"Documentation Database Simplon Introduction This programm create a database to compare its training programs with those of other training centers. To achieve this, the program retrieves training courses from the France Comp\u00e9tences website through its API. Then, the program scrapes the Simplon website to gather information on training courses currently being offered. The API allows querying the created database to find the sought training courses based on the information source (Simplon or France Comp\u00e9tences), the region where the training takes place, and the formacodes. Code Structure azure_resources : creates and deletes Azure resources. azurefunction : this folder contains the various parts of the automation of data retrieval from the France Comp\u00e9tences and Simplon websites. bdd_structure : this folder contains the database schemas. Authentification Endpoints","title":"Documentation Database Simplon"},{"location":"Documentation/introduction/#documentation-database-simplon","text":"","title":"Documentation Database Simplon"},{"location":"Documentation/introduction/#introduction","text":"This programm create a database to compare its training programs with those of other training centers. To achieve this, the program retrieves training courses from the France Comp\u00e9tences website through its API. Then, the program scrapes the Simplon website to gather information on training courses currently being offered. The API allows querying the created database to find the sought training courses based on the information source (Simplon or France Comp\u00e9tences), the region where the training takes place, and the formacodes.","title":"Introduction"},{"location":"Documentation/introduction/#code-structure","text":"azure_resources : creates and deletes Azure resources. azurefunction : this folder contains the various parts of the automation of data retrieval from the France Comp\u00e9tences and Simplon websites. bdd_structure : this folder contains the database schemas.","title":"Code Structure"},{"location":"Documentation/introduction/#authentification","text":"","title":"Authentification"},{"location":"Documentation/introduction/#endpoints","text":"","title":"Endpoints"},{"location":"Documentation/API/code_structure/","text":"Code Structure","title":"Code structure"},{"location":"Documentation/API/code_structure/#code-structure","text":"","title":"Code Structure"},{"location":"Documentation/Automation/code_structure/","text":"Code Structure azurefunction : this folder contains the various parts of the Azure function execution. - CSVtoPostgresDataPipeline : - data : this folder contains the file V12_V13.xls, which lists the formacodes and their descriptions. - main.py : this file calls the following files: - download_file.py : retrieves training courses from France Comp\u00e9tences in CSV format. - fill_database.py : creates SQL database tables with SQLAlchemy and populates the tables. - models : this folder contains the database modeling files: - parents.py : models the tables. - common_imports.py : connects to the database. - association_table.py : defines the composition of association tables. - simplonscrapy : this folder contains the entire Scrapy framework structure. - simplonscrapy : - spiders : this folder contains the scraping file simplonspider.py . - database.py : connects to the database. - items.py : defines the items to be transferred to the database. - middlewares.py : defines and configures components that intercept and modify requests and responses throughout the scraping process. - pipelines.py : cleans the data and loads it into the database. - settings.py : adjusts the scraping settings. - Dockerfile : dockerizes the automation of scraping and database loading. - function_app.py : script for the function running in Azure that automates tasks. - host.json : JSON configuration file for function_app. - requirements.txt : dependencies file.","title":"Code structure"},{"location":"Documentation/Automation/code_structure/#code-structure","text":"azurefunction : this folder contains the various parts of the Azure function execution. - CSVtoPostgresDataPipeline : - data : this folder contains the file V12_V13.xls, which lists the formacodes and their descriptions. - main.py : this file calls the following files: - download_file.py : retrieves training courses from France Comp\u00e9tences in CSV format. - fill_database.py : creates SQL database tables with SQLAlchemy and populates the tables. - models : this folder contains the database modeling files: - parents.py : models the tables. - common_imports.py : connects to the database. - association_table.py : defines the composition of association tables. - simplonscrapy : this folder contains the entire Scrapy framework structure. - simplonscrapy : - spiders : this folder contains the scraping file simplonspider.py . - database.py : connects to the database. - items.py : defines the items to be transferred to the database. - middlewares.py : defines and configures components that intercept and modify requests and responses throughout the scraping process. - pipelines.py : cleans the data and loads it into the database. - settings.py : adjusts the scraping settings. - Dockerfile : dockerizes the automation of scraping and database loading. - function_app.py : script for the function running in Azure that automates tasks. - host.json : JSON configuration file for function_app. - requirements.txt : dependencies file.","title":"Code Structure"},{"location":"Documentation/Automation/scrapy/","text":"Documentation of Scrapy Module Generates initial requests to start scraping using the URLs defined in start_urls . Uses Playwright to handle JavaScript content. Source code in azurefunction/simplonscrapy/simplonscrapy/spiders/simplonspider.py 22 23 24 25 26 27 28 def start_requests ( self ): \"\"\" Generates initial requests to start scraping using the URLs defined in `start_urls`. Uses Playwright to handle JavaScript content. \"\"\" for url in self . start_urls : yield scrapy . Request ( url , meta = { \"playwright\" : True }) # Utilisation de playwright pour JavaScript","title":"Documentation of Scrapy Module"},{"location":"Documentation/Automation/scrapy/#documentation-of-scrapy-module","text":"Generates initial requests to start scraping using the URLs defined in start_urls . Uses Playwright to handle JavaScript content. Source code in azurefunction/simplonscrapy/simplonscrapy/spiders/simplonspider.py 22 23 24 25 26 27 28 def start_requests ( self ): \"\"\" Generates initial requests to start scraping using the URLs defined in `start_urls`. Uses Playwright to handle JavaScript content. \"\"\" for url in self . start_urls : yield scrapy . Request ( url , meta = { \"playwright\" : True }) # Utilisation de playwright pour JavaScript","title":"Documentation of Scrapy Module"}]}